Ensuring language model safety increasingly hinges on the ability to refuse harmful requests—those involving unsafe, illegal, or malicious content—while remaining helpful and accurate on benign prompts~\citep{ma2025safetyscalecomprehensivesurvey}. However, current models suffer from over-refusal, where even harmless or ambiguous inputs are unnecessarily rejected, which reduces model usability. Alignment methods such as RLHF~\citep{ouyang2022traininglanguagemodelsfollow} and Constitutional AI~\citep{bai2022constitutionalaiharmlessnessai} help models follow safety guidelines, but do not adequately address the rising issue of over-refusal on benign prompts. For example, \textsc{Llama-3 8B} exhibits a high over-refusal rate of $\approx 0.69$~\citep{cui2025orbenchoverrefusalbenchmarklarge}.

Although some recent efforts attempt to control refusal behavior through binary harmful/benign distinctions~\citep{arditi2024refusallanguagemodelsmediated}, they fail to capture fine-grained intent, overlook category-specific refusal mechanisms, and struggle with ambiguous commands where harmfulness is context-dependent~\citep{vonrecum2024notautomaticanalysisrefusal}. To address this, \citet{jain2024refusaltokenssimpleway} fine-tune \textsc{Llama-3 8B Base} to generate either (1) a ``[respond]'' token following a normal response to the query, or (2) a categorical ``[refuse]'' token with a refusal message. These tokens belong to one of the five types of refusal defined in~\citet{brahman2024artsayingnocontextual}, such as \textit{Requests with Safety Concerns} and \textit{Incomplete Requests}. This enables more nuanced behavior by allowing the model to distinguish between different types of harmful prompts. 

In this ongoing work, we take a first step toward examining whether categorical refusal tokens enable more interpretable and controllable model behavior. We analyze their internal representations, identify residual-stream features associated with each type of refusal, and leverage them to steer model responses at inference. Our contributions are: (1) extract category-specific refusal steering vectors; (2) empirical evidence that our categorical steering reduces over-refusal on ambiguous and benign prompts while preserving refusal on harmful ones across safety benchmarks; and (3) analysis showing that the identified refusal features are distinct, interpretable, and arise from refusal-token fine-tuning.


% Our goal is to assess the effectiveness of token-based refusal as a fine-grained control mechanism to reduce over-refusal without compromising safety.
% We further demonstrate that the identified refusal features emerge from refusal token fine-tuning by comparing them with similarly identified features on the base \textsc{Llama-3 8B}.

% Ensuring the safety and reliability of language models has become a central research priority~\citep{ma2025safetyscalecomprehensivesurvey}, especially their ability to refuse harmful requests while remaining helpful on benign prompt. However, models suffer from \textit{over-refusal}, where harmless or ambiguous prompts are unnecessarily refused, reducing their usability. Several past works have focused on aligning language models with human preferences and expectations, including refusing to answer specific questions based on safety guidelines, through Reinforcement Learning from Human Feedback (RLHF) ~\citep{ouyang2022traininglanguagemodelsfollow} and supervised rule-based methods, such as Constitutional AI ~\citep{bai2022constitutionalaiharmlessnessai}. However, none of these methods address the increasing over-refusal rates. For example, \textsc{Llama-3 8B} exhibits a high over-refusal rate of $\approx 0.69$~\citep{cui2025orbenchoverrefusalbenchmarklarge}. Mechanistic interpretability methods ~\citep{rai2025practicalreviewmechanisticinterpretability} aim to understand the internal computations that drive the behavior of models. Existing approaches for controlling refusal via binary harmful vs. benign signals (e.g., ~\citet{arditi2024refusallanguagemodelsmediated}), overlook fine-grained intent of harmful prompts, fail to isolate features specific to different refusal types, and worsen both interpretability and under-refusal on ambiguous prompts where malicious intents appear benign (~\citep{vonrecum2024notautomaticanalysisrefusal}). To address this, \citet{jain2024refusaltokenssimpleway} fine-tune \textsc{Llama-3 8B} to generate categorical “[refuse]” or “[respond]” tokens, followed by a refusal message or a normal response. There are  five refusal categories that are drawn from~\citet{brahman2024artsayingnocontextual}, enabling for fine-grained refusal categories and a nuanced understanding of refusal behavior.

% In this work, we build on this fine-tuned model to interpret and steer category-specific refusal behavior, aiming to reduce over-refusal while preserving safety and accuracy. By identifying the residual-stream features linked to each refusal category, we can steer fine-grained refusals at inference and better understand the specific residual stream features that cause language models to refuse to answer different categories of harmful prompts. We further show that the discovered features are unique to the refusal token fine-tuned model when compared to the base \textsc{Llama-3 8B}.

% Our contributions are: (i) a simple procedure to extract category-specific sparse steering vectors from residual-stream activations; (ii) empirical evidence that categorical steering reduces over-refusal on ambiguous and benign prompts while maintaining refusal rates on genuinely harmful prompts across safety benchmarks; and (iii) analysis demonstrating that the identified refusal features are distinct and interpretable, emerge from refusal-token fine-tuning, and enable downstream steering without degrading general task performance.