\paragraph{Analysis on Category-Specific Steering Vectors and Features}

We steer at the residual stream after the MLP in layer $9$; we selected this site empirically based on preliminary exploration and due to computational constraints.
The computed pairwise cosine similarities between the five category-specific steering vectors at layer 9 have generally low-to-moderate values (Figure~\ref{fig:steering_vector_cos_sim} in Appendix~\ref{app:steering_vector_cos_sim}), indicating partial decorrelation that makes the steering vectors suitable for fine-grained steering control. Notably, the \emph{Incomplete} steering vector is especially decorrelated, indicating that the features for mediating refusal for incomplete requests are unique. We also find that features $4055$ and $290$ are consistently the most active across the steering vectors (Figure~\ref{fig:identified_top_features} in Appendix~\ref{app:identified-features}).

% Geometry diagnostics over the categorical harmful and benign residual-stream activations yield Silhouette Score = $-0.028$, Davies--Bouldin = $10.31$, and Calinski--Harabasz = $0.0$, suggesting weak separability at this site and token position; nevertheless, the cosine structure affords usable steering.

\paragraph{Do Refusal Token Fine-Tuning Induce Emergent Category-Specific Features?}

To validate that our identified refusal features emerge from refusal token fine-tuning, we evaluate the exclusiveness of features from the refusal token fine-tuned model when compared to the base \textsc{Llama-3 8B}. Using model diffing, we compute steering vectors using the same methodology on both models and compute cosine similarities between pairs of steering vectors. Lower cosine similarity values generally indicate that the corresponding features are likely emergent from fine-tuning.

% \begin{wraptable}{r}{0.5\textwidth}
\begin{table}
% \vspace{-1.2em}
\centering
\caption{Model diffing cosine similarities.}
% \resizebox{0.95\linewidth}{!}{
\begin{tabular}{lc}
\toprule
Category & Cosine Sim \\
\midrule
Requests with safety concern & 0.336 \\
Humanizing requests & 0.317 \\
Incomplete requests & 0.651 \\
Unsupported requests & 0.333 \\
Indeterminate requests & 0.334 \\
\bottomrule
\end{tabular}
% }
\label{tab:model-diffing-cosine-sims}
% \vspace{-0.9em}
\end{table}

Across most categories, cross-model similarities are low (0.317 -- 0.336), while \emph{Incomplete} shows a higher alignment (0.651) (Table~\ref{tab:model-diffing-cosine-sims}), suggesting partial reuse of base model features in that case. Overall, this pattern of low-to-moderate similarity supports the hypothesis that refusal-token fine-tuning induces novel, category-specific, refusal-mediating features.

\paragraph{Can Categorical Steering Reduce Over-Refusal Without Compromising Safety?}

We evaluate refusal behavior and safety performance across \textsc{Llama-3 8B Base}, the binary and categorical refusal-token–fine-tuned model, and our categorically steered model. On all three benchmarks, we see that steering significantly reduces over-refusal on ambiguous and benign prompts while preserving the refusal rate on truly harmful requests. Specifically, on CoCoNot Contrast (benign but ambiguous prompts), over-refusal drops from 0.106 to 0.0 with steering, while refusal on CoCoNot Orig (harmful prompts) increases from 0.666 to 0.716 (Table~\ref{tab:safety-results}). Similar trends hold on WildJailbreak and OR-Bench.

% \paragraph{CoCoNot} We evaluate refusal rates on CoCoNot Orig (harmful) and CoCoNot Contrast (ambiguous, benign).

\begin{table}[H]
\centering
\caption{Refusal rates across safety benchmarks, grouped by benign vs. harmful.}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
Dataset & \textsc{Llama-3 8B Base} & \shortstack{Binary\\Tokens FT} & \shortstack{Categorical\\Tokens FT} & \textbf{\shortstack{Categorically\\Steered (Ours)}} \\
\midrule
\multicolumn{5}{l}{\emph{Benign prompts (lower is better)}} \\
\addlinespace
CoCoNot Contrast (Benign) & 0.045 & 0.124 & 0.106 & \textbf{0.0} \\
WildJailbreak Adversarial Benign & 0.148 & 0.138 & 0.086 & \textbf{0.0} \\
OR-Bench Hard (Benign) & 0.180 & 0.497 & 0.388 & \textbf{0.010} \\
\midrule
\multicolumn{5}{l}{\emph{Harmful prompts (higher is better)}} \\
\addlinespace
CoCoNot Orig (Harmful) & 0.198 & 0.715 & 0.666 & \textbf{0.716} \\
WildJailbreak Adversarial Harmful & \textbf{0.565} & 0.245 & 0.222 & 0.225 \\
OR-Bench Toxic (Harmful) & 0.214 & 0.685 & 0.785 & \textbf{0.789} \\
\bottomrule
\end{tabular}
}
\label{tab:safety-results}
\end{table}

% Additionally, we report results for \textsc{Zephyr-Llama3 8B SFT}, which reduces benign over-refusal relative to the FT baseline but remains above our steered model (CoCoNot-Contrast 0.098; WJ-Benign 0.171; OR-Bench Hard 0.497), while maintaining high refusal on harmful prompts (OR-Bench Toxic 0.904; WJ-Harmful 0.389).

% On CoCoNot Contrast, the over-refusal rate drops (0.106,$\to$\,0.0) on ambiguous prompts (Tab.~\ref{tab:coconot}). On CoCoNot Orig, the refusal rate increases (0.666,$\to$\,0.716) on harmful prompts (Tab.~\ref{tab:coconot}).

% Results for WildJailbreak and OR-Bench are included in Table~\ref{tab:safety-results}.

\paragraph{Does Categorical Steering Preserve General Model Performance?}

% \paragraph{General Performance Metrics}
As shown in Table~\ref{tab:general-performance}, the steered model achieves identical accuracy to the refusal-token–fine-tuned model across all three general benchmarks: MMLU, GSM8k, and TruthfulQA.

\begin{table}[H]
\centering
\caption{General Performance Metrics .}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{llccc}
\toprule
Dataset & \textsc{Llama-3 8B Base} & \shortstack{Binary\\Tokens FT} & \shortstack{Categorical\\Tokens FT} & \textbf{\shortstack{Categorically\\Steered (Ours)}} \\
\midrule
MMLU & $0.6206\pm0.0038$ & $0.5861\pm0.0039$ & $0.5887\pm0.0039$ & $0.5887\pm0.0039$ \\
GSM8k & $0.5057\pm0.0138$ & $0.4496\pm0.0137$ & $0.4534\pm0.0137$ & $0.4534\pm0.0137$ \\
% GSM8k Strict Match & $0.5034\,\pm\,0.0138$ & $0.4526\,\pm\,0.0137$ & $0.4526\,\pm\,0.0137$ \\
TruthfulQA MC & $0.2717\pm0.0156$ & $0.3158\pm0.0163$ & $0.3158\pm0.0163$ & $0.3158\pm0.0163$ \\
% TruthfulQA MC 2 & $0.4399\,\pm\,0.0139$ & $0.4568\,\pm\,0.0148$ & $0.4811\,\pm\,0.0150$ & $0.4811\,\pm\,0.0150$ \\
\bottomrule
\end{tabular}
}
\label{tab:general-performance}

\end{table}

% On general benchmarks, \textsc{Zephyr-Llama3 8B SFT} is comparable to the FT and steered models (e.g., MMLU 0.5916, GSM8k 0.4594, TruthfulQA MC1/MC2 0.3011/0.4568; Table~\ref{tab:general-performance}).