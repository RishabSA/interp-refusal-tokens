% Our objective is to interpret and control fine-grained categories of refusal behavior in language models, to reduce over-refusal while maintaining appropriate refusal on harmful prompts and preserving model performance on benign inputs. We also investigate whether the features associated with each harmful category are unique to the model fine-tuned with refusal tokens, compared to the base \textsc{Llama-3 8B}, through model diffing.

Our methodology involves extracting category-specific features, constructing sparse steering vectors, applying them at inference time, and comparing representational differences with a \textsc{Llama-3 8B Base} model via model diffing. We demonstrate our framework in Figure~\ref{fig:refusal_token_methodology}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{visuals/framework.pdf}
    \caption{Our framework of activation extraction, steering vector computation, and inference-time categorical steering}
    \label{fig:refusal_token_methodology}
\end{figure}

\paragraph{Caching Activations}
Using the fine-tuned refusal token model from~\citet{jain2024refusaltokenssimpleway}, we first extract residual-stream activations at a given layer $l$. Specifically, we target the post-MLP activation for the final token in each input sequence. We experiment with different layers to maximize separation between activations of various categories and to provide the best steering capabilities at inference.

For each of the five harmful categories of prompts and the benign category of prompts, we hook into the model at layer $l$ and extract the residual-stream activation for the last token in each prompt. We then compute mean activations $\mu^l_c$ for each harmful category $c$ and $\mu^l_b$ for the benign category $b$.

% We cache these activations for each harmful category $c$ and use them to calculate the mean activations $\mu^l_c$ for each harmful category. The same is done for the benign category $b$.

\paragraph{Identifying Features and Steering Vectors} \label{sec:compute_features_vectors}
% We apply a methodology similar to 
% those of \textit{Contrastive Activation Addition} ~\citep{panickssery2024steeringllama2contrastive} and 
% \textit{Sparse Activation Steering (SAS)} ~\citep{bayat2025steeringlargelanguagemodel}. 
We apply a similar method of \textit{Sparse Activation Steering (SAS)} ~\citep{bayat2025steeringlargelanguagemodel}, directly to the residual-stream activations of the model rather than a latent autoencoder representation. To construct a steering vector for category $c$, we first threshold the mean activation $\mu^l_c$, retaining only features above a fixed threshold $\tau$, resulting in a filtered mean activation $\tilde{\mu}^l_c$.
% For each of the categories $c$, we begin to construct steering vectors by filtering out features from the mean activations $\mu^l_c$ that are lower than a threshold hyperparameter $\tau$. 
For each harmful category, we compute a steering vector by subtracting the benign category's mean activation from the harmful category's mean activation:
% \begin{equation}
$v_c^l = \tilde{\mu}^l_c - \tilde{\mu}^l_b$.
% \end{equation}
Then, we enforce sparsity in the steering vectors by only keeping the top-$K$ features from each of the category-specific steering vectors, creating $\tilde{v}_c^l$. This is to ensure that steering does not affect general model capabilities.
Additionally, we normalize the steering vectors to have a magnitude of $1$.

\paragraph{Steering Refusal Behavior}

Using the identified steering vectors, we steer model refusal behavior at inference time with the goal of reducing over-refusal while maintaining high refusal rates on genuinely harmful prompts. For each newly generated token, we add the corresponding category-specific steering vector $\tilde{v}_c^l$ to the residual stream activation of the final token at a designated layer $l$. We also apply a strength hyperparameter $\alpha$ to control the magnitude and direction of the intervention:
% \begin{equation}
$\tilde z^{l} = z^{l} + \alpha \ \tilde{v}_c^l$.
% \end{equation}
A positive $\alpha$ amplifies refusal behavior on harmful prompts, while a negative $\alpha$ reduces refusal on benign and ambiguous prompts, thereby reducing over-refusal.

Steering is applied categorically based on the contents of the input prompt. The model selects the most optimal steering vector for application at inference time. This process works by first generating a "[refuse]" or "[respond]" token without any steering, and then using the generated refusal token as a key to map to its corresponding category's specific steering vector $\tilde{v}_c^l$ and steering strength $\alpha$ to steer fine-grained refusal behavior.