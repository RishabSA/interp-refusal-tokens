\paragraph{Model Refusal} Language models are designed to refuse to respond with unsafe, illegal, harmful, or otherwise malicious responses. Several past studies have focused on aligning language models with human preferences and expectations, including refusing to answer specific questions based on safety guidelines, through Reinforcement Learning from Human Feedback (RLHF) ~\citep{ouyang2022traininglanguagemodelsfollow} and supervised rule-based methods, such as Constitutional AI ~\citep{bai2022constitutionalaiharmlessnessai}.\citet{jain2024refusaltokenssimpleway} fine-tune \textsc{Llama-3 8B} to produce a category-specific refusal or response token at the beginning of model responses based on the content of the user's prompt. However, none of these methods address the increasing over-refusal rates. We employ steering techniques with a fine-grained level of control using specific harmful categories to reduce the over-refusal rate while maintaining the refusal rate on truly harmful prompts.

\paragraph{Understanding and Steering LLM Refusal} Mechanistic interpretability methods ~\citep{rai2025practicalreviewmechanisticinterpretability} aim to understand the internal computations that drive the behavior of models. Previous works have focused on identifying features for simple harmful vs. benign binary tasks, such as ~\citet{arditi2024refusallanguagemodelsmediated} and ~\citet{panickssery2024steeringllama2contrastive}. These binary features are used in steering vectors to steer model behavior towards refusals or responses. However, these methods fail to identify specific features for different types of refusals. We aim to better understand the particular residual stream features that cause language models to refuse to answer different categories of harmful prompts, and to use these features in steering vectors to steer refusal behavior at fine-grained levels.

% \textbf{Sparse Autoencoders.} LLMs tend to encode more features than their dimensions can represent. This creates an issue known as superposition (Elhage et al., 2022), where unrelated concepts become entangled in the model's representation space. When a single neuron responds to multiple unrelated features (polysemanticity), it becomes more difficult to isolate and interpret individual concepts and behaviors, such as refusal. Sparse Autoencoders (SAEs) are neural networks designed to address this issue, mapping model activations into a higher-dimensional space with enforced sparsity via penalties, ensuring only a few neurons fire for each input (Cunningham et al., 2023). This creates a set of disentangled and thus interpretable features that can linearly reconstruct the original activations (Templeton et al., 2024). For example, Yeo et al. (2024) use SAEs to analyze refusal in instruction-tuned LLMs, focusing on simple, binary refusal versus acceptance, but without addressing more nuanced cases, such as ambiguous prompts. 

% Another example of causal intervention is patching. Activation patching ~\citep{zhang2024bestpracticesactivationpatching} involves taking hidden states from a "source" forward pass (for example, when the model complies) and injecting them into a "target" run with different behavior to observe changes in the target output. This replacement helps test the causal role of internal activations ~\citep{vig2020causalmediationanalysisinterpreting}. It has been applied at different levels of granularity, from individual attention heads ~\citep{Elhage_Nanda_2021} to full MLP layers~\citep{geiger2025causalabstractiontheoreticalfoundation}.
% Attribution patching ~\citep{syed2023attributionpatchingoutperformsautomated} is a technique that uses gradients to approximate activation patching more efficiently, ranking components based on their causal effect on a specific output and then selectively patching only the most influential ones. It is more scalable for large models ~\citep{kram√°r2024atpefficientscalablemethod} and more useful for hypothesis-driven scenarios.
%

% Most of the field of mechanistic interpretability revolves around causal intervention, or testing whether identified individual components are responsible (in a cause-and-effect relationship) for certain behaviors, specifically by intervening in activations or structure. One method of causal intervention is ablation studies ~\citep{meyes2019ablationstudiesartificialneural}, where specific components (e.g., attention heads or even full layers) are disabled to test how they individually contribute to end behavior ~\citep{li2017understandingneuralnetworksrepresentation}. For example, ~\citet{michel2019sixteenheadsreallybetter} disables a large percentage of attention heads used during model training, without retraining the model, and discovers little to no impact on model performance; they describe this process as "pruning down models". Similarly, ~\citet{zhou2018revisitingimportanceindividualunits} turns off individual units or neurons in an image recognition Convolutional Neural Network (CNN), finding a significant drop in class-specific accuracy but not overall vision recognition accuracy. 